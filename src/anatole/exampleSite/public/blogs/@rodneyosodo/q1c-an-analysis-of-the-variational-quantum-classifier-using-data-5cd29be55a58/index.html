<!doctype html><html dir=ltr lang=en data-theme class=html><head><title>Osodo Rodney
|
Q1c. An analysis of the variational quantum classifier using data</title><meta charset=utf-8><meta name=generator content="Hugo 0.109.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content="Osodo Rodney"><meta name=description content="
      Our previous blog is at: “Explaining Variational Quantum Classifiers”


    "><meta name=google-site-verification content="rodneyosodo.com"><link rel=stylesheet href=/scss/main.min.b2e0cb07595e3519ab1193bb421914e06c0e26b0cc561fef23b3c6131d4d2ffa.css integrity="sha256-suDLB1leNRmrEZO7QhkU4GwOJrDMVh/vI7PGEx1NL/o=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.31b0a1f317f55c529a460897848c97436bb138b19c399b37de70d463a8bf6ed5.css integrity="sha256-MbCh8xf1XFKaRgiXhIyXQ2uxOLGcOZs33nDUY6i/btU=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/fontawesome.min.b1c4e6a10bdbab01f33fff9d78816ee68cf9a9a731f07668afd546a79924cb80.css integrity="sha256-scTmoQvbqwHzP/+deIFu5oz5qacx8HZor9VGp5kky4A=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.423dee17c62f55fa733a4ee13e00d523dfce88cc4f4ab4549a24ba36bd9de681.css integrity="sha256-Qj3uF8YvVfpzOk7hPgDVI9/OiMxPSrRUmiS6Nr2d5oE=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.b7d54133b27e5b4de15245b8e143de3e8ed2d674c706137274cedc9953f31917.css integrity="sha256-t9VBM7J+W03hUkW44UPePo7S1nTHBhNydM7cmVPzGRc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicons/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicons/favicon-16x16.png><link rel=canonical href=https://rodneyosodo.com/blogs/@rodneyosodo/q1c-an-analysis-of-the-variational-quantum-classifier-using-data-5cd29be55a58/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.738c0e3a493854876aeab9e2316fd43f1936aeeac4cc6b3e60bb26456dba72ad.js integrity="sha256-c4wOOkk4VIdq6rniMW/UPxk2rurEzGs+YLsmRW26cq0=" crossorigin=anonymous></script><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rodneyosodo.com/images/site-feature-image.png"><meta name=twitter:title content="Q1c. An analysis of the variational quantum classifier using data"><meta name=twitter:description content="Our previous blog is at: “Explaining Variational Quantum Classifiers”"><meta property="og:title" content="Q1c. An analysis of the variational quantum classifier using data"><meta property="og:description" content="Our previous blog is at: “Explaining Variational Quantum Classifiers”"><meta property="og:type" content="article"><meta property="og:url" content="https://rodneyosodo.com/blogs/@rodneyosodo/q1c-an-analysis-of-the-variational-quantum-classifier-using-data-5cd29be55a58/"><meta property="og:image" content="https://rodneyosodo.com/images/site-feature-image.png"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2021-01-22T18:16:28+00:00"><meta property="article:modified_time" content="2021-01-22T18:16:28+00:00"><meta property="og:site_name" content="Rodney Osodo"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"blogs","name":"Q1c. An analysis of the variational quantum classifier using data","headline":"Q1c. An analysis of the variational quantum classifier using data","alternativeHeadline":"","description":"
      Our previous blog is at: “Explaining Variational Quantum Classifiers”


    ","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/rodneyosodo.com\/blogs\/@rodneyosodo\/q1c-an-analysis-of-the-variational-quantum-classifier-using-data-5cd29be55a58\/"},"author":{"@type":"Person","name":"Osodo Rodney"},"creator":{"@type":"Person","name":"Osodo Rodney"},"accountablePerson":{"@type":"Person","name":"Osodo Rodney"},"copyrightHolder":{"@type":"Person","name":"Osodo Rodney"},"copyrightYear":"2021","dateCreated":"2021-01-22T18:16:28.28Z","datePublished":"2021-01-22T18:16:28.28Z","dateModified":"2021-01-22T18:16:28.28Z","publisher":{"@type":"Organization","name":"Osodo Rodney","url":"https://rodneyosodo.com","logo":{"@type":"ImageObject","url":"https:\/\/rodneyosodo.com\/favicons\/favicon-32x32.png","width":"32","height":"32"}},"image":["https://rodneyosodo.com/images/site-feature-image.png"],"url":"https:\/\/rodneyosodo.com\/blogs\/@rodneyosodo\/q1c-an-analysis-of-the-variational-quantum-classifier-using-data-5cd29be55a58\/","wordCount":"1727","genre":[],"keywords":[]}</script></head><body class="body theme--light"><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/images/profile.png alt="profile picture"><div class=sidebar__introduction-title><a href=/>Rodney Osodo</a></div><div class=sidebar__introduction-description><p>This is Rodney Osodo's website</p></div></div><ul class=sidebar__list><li class=sidebar__list-item><a href=https://www.linkedin.com/in/rodneyosodo rel=me aria-label=Linkedin title=Linkedin><i class="fab fa-linkedin fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://github.com/rodneyosodo rel=me aria-label=GitHub title=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://gitlab.com/0x6f736f646f rel=me aria-label=Gitlab title=Gitlab><i class="fab fa-gitlab fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://twitter.com/b1ackd0t rel=me aria-label=Twitter title=Twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://www.instagram.com/rodneyosodo/ rel=me aria-label=Instagram title=Instagram><i class="fab fa-instagram fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://www.tiktok.com/@b1ackd0t rel=me aria-label=TikTok title=TikTok><i class="fab fa-tiktok fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://www.youtube.com/@rodneyosodo rel=me aria-label=Youtube title=Youtube><i class="fab fa-youtube fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://stackoverflow.com/users/9017060/black-dot rel=me aria-label=StackOverflow title=StackOverflow><i class="fab fa-stack-overflow fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://rodneyosodo.medium.com/ rel=me aria-label=Medium title=Medium><i class="fab fa-medium fa-2x" aria-hidden=true></i></a></li><li class=sidebar__list-item><a href=https://open.spotify.com/show/2MBoVx1mlX0kjHVrooqasf rel=me aria-label=Spotify title=Spotify><i class="fab fa-spotify fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
2020-2023</li><li class=footer__item><a class=link href=/imprint/ title>imprint</a></li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script><script defer type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity=sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN crossorigin=anonymous></script>
<script type=text/x-mathjax-config>
      MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu><li class=nav__list-item><a href=/ title>Home</a></li><li class=nav__list-item><a href=/blogs/ title>Blogs</a></li><li class=nav__list-item><a href=/portfolio/ title>Portfolio</a></li><li class=nav__list-item><a href=/publications/ title>Publications</a></li><li class=nav__list-item><a href=/talks/ title>Talks</a></li><li class=nav__list-item><div class=optionswitch><input class=optionswitch__picker type=checkbox id=menuoptionpicker hidden>
<label class=optionswitch__label class=optionswitch__label for=menuoptionpicker>Accomplishments <i class="fa fa-angle-down" aria-hidden=true></i></label><div class=optionswitch__triangle></div><ul class=optionswitch__list><li class=optionswitch__list-item><a href=/awards/ title>Awards</a></li><li class=optionswitch__list-item><a href=/certifications/ title>Certifications</a></li></ul></div></li><li class=nav__list-item><a href=/contact/ title>Contact</a></li></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>Q1c. an Analysis of the Variational Quantum Classifier Using Data</h1><blockquote><p><strong>Our previous blog is at: “</strong><a href="https://rodneyosodo.medium.com/qa2-explaining-variational-quantum-classifiers-b584c3bd7849?source=friends_link&sk=9ccabff95cb8c2ffa7c8c32bd5424a39"><strong>Explaining Variational Quantum Classifiers</strong></a><strong>”</strong></p></blockquote><p>By now you should know how a variational quantum classifier works. The code for the previous series is at <a href=https://github.com/0x6f736f646f/variational-quantum-classifier-on-heartattack>Github repo</a></p><h3 id=introduction>Introduction</h3><p>In binary classification, let’s say labelling if someone is likely to have a heart attack or not, we would build a function that takes in the information about the patient and gives results aligned to the reality. E.g,</p><p><img src=/images/blogimages/1__eoXLYJdN3tBlRodKt2__jsw.png alt></p><p>This probabilistic classification is well suited for quantum computing and we would like to build a quantum state that, when measured and post processed, returns</p><p><img src=/images/blogimages/1__7gItUIkHJNMFiJ3cLTeqxA.png alt></p><p><code>P(hear attack = YES)</code></p><p>By optimizing the circuits, you then find parameters that will give the closest probability to the reality based on training data.</p><h3 id=problem-statement>Problem statement</h3><p>Given a dataset about patients’ information, can we predict if the patient is likely to have a heart attack or not? This is a binary classification problem, with a real input vector <code>{x}</code> and a binary output <code>{y}</code> in <code>{0, 1}</code>. We want to build a quantum circuit whose output is the quantum state:</p><p><img src=/images/blogimages/1__7Zuf0__DQI6lQRX__OznASHw.png alt></p><h3 id=implementation>Implementation</h3><ol><li>We initialize our circuit in the zero state (all qubits in state zero)</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>self.sv = Statevector.from_label(&#39;0&#39; * self.no_qubit)
</span></span></code></pre></div><p>2. We use a feature map such as, <code>ZZFeaturemap, ZFeaturemap</code> or <code>PauliFeaturemap</code> and choose the number of qubits based on the input dimension of the data and how many repetitions (i.e. the circuit depth) we want. We use 1, 3, 5.</p><p>3. We choose the variational form as <code>RealAmplitudes</code> and specify the number of qubits as well as how many repetitions we want. We use 1, 2, 4 to have models with an increasing number of trainable parameters.</p><p>4. We then combine our feature map to the variational circuit. <code>ZZfeaturemap and RealAmplitudes both with a depth of 1</code></p><p>def prepare_circuit(self):<br>"""<br>Prepares the circuit. Combines an encoding circuit, feature map, to a variational circuit, RealAmplitudes<br>:return:<br>"""<br>self.circuit = self.feature_map.combine(self.var_form)</p><p>5. We create a function that associates the parameters of the feature map with the data and the parameters of the variational circuit with the parameters passed. This is to ensure in Qiskit that the right variables in the circuit are associated with the right quantities.</p><p>def get_data_dict(self, params, x):<br>"""<br>Assign the params to the variational circuit and the data to the featuremap<br>:param params: Parameter for training the variational circuit<br>:param x: The data<br>:return parameters:<br>"""<br>parameters = {}<br>for i, p in enumerate(self.feature_map.ordered_parameters):<br>parameters[p] = x[i]<br>for i, p in enumerate(self.var_form.ordered_parameters):<br>parameters[p] = params[i]<br>return parameters</p><p><img src=/images/blogimages/1__ZhVdtLXyVygYNbDA1j0t4g.png alt></p><p>6. We create another function that checks the parity of the bit string passed. If the parity is even, it returns a ‘yes’ label and if the parity is odd it returns a ‘no’ label. We chose this since we have 2 classes and parity checks either returns true or false for a given bitstring. There are also other methods e.g for 3 classes you might convert the bistring to a number and pass is through an activation function. Or perhaps interpret the expectation values of a circuit as probabilities. The important thing to note is that there are multiple ways to assign labels from the output of a quantum circuit and you need to justify why or how you do this. In our case, the parity idea was originally motivated in this very nice paper (<a href=https://arxiv.org/abs/1804.11326>https://arxiv.org/abs/1804.11326</a>) and the details are contained therein.</p><p>7. Now we create a function that returns the probability distribution over the model classes. After measuring the quantum circuit multiple times (i.e. with multiple shots), we aggregate the probabilities associated with ‘yes’ and ‘no’ respectively, to get probabilities for each label.</p><p>def return_probabilities(self, counts):<br>"""<br>Calculates the probabilities of the class label after assigning the label from the bit string measured<br>as output<br>:type counts: dict<br>:param counts: The counts from the measurement of the quantum circuit<br>:return result: The probability of each class<br>"""<br>shots = sum(counts.values())<br>result = {self.class_labels[0]: 0, self.class_labels[1]: 0}<br>for key, item in counts.items():<br>label = self.assign_label(key)<br>result[label] += counts[key] / shots<br>return result</p><p>8. Finally, we create a function that classifies our data. It takes in data and parameters. For every data point in the dataset we assign the parameters to the feature map and the parameters to the variational circuit. We then evolve our system and store the quantum circuit. We store the circuits so as to run them at once at the end. We measure each circuit and return the probabilities based on the bit string and class labels.</p><p>def classify(self, x_list, params):<br>"""<br>Assigns the x and params to the quantum circuit the runs a measurement to return the probabilities<br>of each class<br>:type params: List<br>:type x_list: List<br>:param x_list: The x data<br>:param params: Parameters for optimizing the variational circuit<br>:return probs: The probabilities<br>"""<br>qc_list = []<br>for x in x_list:<br>circ_ = self.circuit.assign_parameters(self.get_data_dict(params, x))<br>qc = self.sv.evolve(circ_)<br>qc_list += [qc]<br>probs = []<br>for qc in qc_list:<br>counts = qc.to_counts()<br>prob = self.return_probabilities(counts)<br>probs += [prob]<br>return probs</p><h3 id=results>Results</h3><p>Data classification was performed by using the implemented version of VQC in IBM’s framework and executed on the provider simulator</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>qiskit==0.23.1qiskit-aer==0.7.1qiskit-aqua==0.8.1qiskit-ibmq-provider==0.11.1qiskit-ignis==0.5.1qiskit-terra==0.16.1
</span></span></code></pre></div><p>Every combination of the experiments were executed with 1024 shots, using the implemented version of the optimizers. We conducted tests with different feature maps and depths, the RealAmplitudes variational form with differing depths and different optimizers in Qiskit. In each case, we compared the loss values after 50 training iterations on the training data. Our best model configs were</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>ZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 5 : Cost: 0.13492279429495616ZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 3 : Cost: 0.13842958846394343ZFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 3 : Cost: 0.14097642258192988ZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 1 : Cost: 0.14262128997684975ZFeatureMap(4, reps=1) COBYLA(maxiter=50) vdepth 1 : Cost: 0.1430145495411656ZZFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 5 : Cost: 0.14359757088670677ZFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 5 : Cost: 0.1460568741051525ZFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 3 : Cost: 0.14830080135566964ZFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 5 : Cost: 0.14946706294763648ZFeatureMap(4, reps=1) COBYLA(maxiter=50) vdepth 3 : Cost: 0.15447151389989414
</span></span></code></pre></div><p>From the results, the ZFeatureMap with a depth of 2, RealAmplitudes variational form with a depth of 5 and the SPSA optimizer achieved the lowest cost. These results seem to indicate that the feature map which resulted in a lower cost function generally was the ZFeatureMap. But does this mean that the ZFeaturemap typically performs better in general?</p><h3 id=questions>Questions</h3><h4 id=1-does-increasing-the-variational-form-depth-increase-convergence>1. Does increasing the variational form depth increase convergence?</h4><ul><li>Interestingly, increasing the depth of the variational form does not seem to increase convergence of any of these models substantially. Note that increasing the variational form’s depth implies introducing more trainable parameters into the model. One would naively think that more parameters in the model would allow us to model things better and capture more intricate relationships that exist in the data, but perhaps these models are simply too small to exploit any of these advantages through higher parameterization.</li></ul><h4 id=2-does-increasing-featuremap-depth-increase-convergence>2. Does increasing featuremap depth increase convergence?</h4><ul><li>When increasing feature map depth on <code>ZZFeatureMap ADAM (maxiter=50)</code> and <code>PauliFeatureMap ADAM(maxiter=50)</code>, this does increase the convergence of model training. The other model configs don&rsquo;t change significantly (in some, increasing the feature map depth actually reduces convergences almost linearly - why this happens could make for an interesting research project!).</li></ul><h4 id=3-how-do-the-models-generalize-on-different-datasets>3. How do the models generalize on different datasets?</h4><ul><li>As a final experiment, we bench marked these results on the iris and wine datasets. Two popular datasets used in classical machine learning and of the same dimension of the heart attack data, hence we can also use 4 qubits to model it. This time, the best model configs were:</li></ul><p><strong>Iris dataset</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>PauliFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 3 : Cost: 0.18055905629600544ZZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 5 : Cost: 0.18949957468013437ZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 5 : Cost: 0.18975231416858743ZZFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 3 : Cost: 0.1916829328746686ZZFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 3 : Cost: 0.19264230430490895ZZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 3 : Cost: 0.19356269726482855ZFeatureMap(4, reps=4) COBYLA(maxiter=50) vdepth 1 : Cost: 0.19415440209151674ZZFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 5 : Cost: 0.19598553766368446ZFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 1 : Cost: 0.19703058320810934ZFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 3 : Cost: 0.19970277845347006
</span></span></code></pre></div><p><strong>Wine dataset</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>PauliFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 5 : Cost: 0.1958180042610037PauliFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 3 : Cost: 0.1962278498243972PauliFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 3 : Cost: 0.20178754496022344ZZFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 1 : Cost: 0.20615090555639448PauliFeatureMap(4, reps=2) SPSA(max_trials=50) vdepth 1 : Cost: 0.20621624103441463ZZFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 1 : Cost: 0.20655139975269518PauliFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 1 : Cost: 0.20655139975269518ZZFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 1 : Cost: 0.20655139975269518PauliFeatureMap(4, reps=2) COBYLA(maxiter=50) vdepth 1 : Cost: 0.20655139975269518ZFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 5 : Cost: 0.20674662980116945ZFeatureMap(4, reps=1) SPSA(max_trials=50) vdepth 5 : Cost: 0.2076046292803965ZZFeatureMap(4, reps=4) SPSA(max_trials=50) vdepth 5 : Cost: 0.20892451316076094
</span></span></code></pre></div><h3 id=discussion>Discussion</h3><p>This time, our best model configs are totally different! What’s fascinating about this is that the dataset used seems to demand a particular model structure. This makes sense intuitively right? Because the first step in these quantum machine learning models is to load the data and encode it into a quantum state. If we use different data, perhaps there is a different (or more optimal) data encoding strategy depending on the kind of data you have.</p><p>Another thing that surprised me, especially coming from a classical ML background, is the performance of the SPSA optimizer. I would have thought something more state-of-the-at, like ADAM, would be the clear winner. This was not the case at all. It would be cool to understand why SPSA seems to well suited for optimizing these quantum models.</p><p>A final remark is that we only looked at the loss values on training data. Ultimately we would like to also see if any of these quantum models are good at generalization. A model is said to have good generalization if it is capable of performing well on new data that it has never seen before. A proxy for this is usually the error we would get on test data. By taking the best configs here and checking their performance on test sets, we could gauge how well these toy models perform and generalize which would be pretty interesting even in these small examples!</p><p>We are now (sadly!) at the finishing line. We have come so far and there are still many more open questions to uncover. If you are interested in any of this work, please feel free to reach out and maybe we could collaborate on something cool! Hopefully, you have understood the pipeline of training a quantum machine learning algorithm using real world data. Thank you for reading these posts and thanks to <a href="https://scholar.google.com/citations?user=-v3wO_UAAAAJ">Amira Abbas</a> for mentoring me through the QOSF program. Until next time 👋</p></div><div class=post__footer></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
2020-2023</li><li class=footer__item><a class=link href=/imprint/ title>imprint</a></li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script><script defer type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML" integrity=sha384-e/4/LvThKH1gwzXhdbY2AsjR3rm7LHWyhIG5C0jiRfn8AN2eTN5ILeztWw0H9jmN crossorigin=anonymous></script>
<script type=text/x-mathjax-config>
      MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script></body></html>